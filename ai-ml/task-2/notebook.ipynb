{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvkxJLK872xG"
      },
      "source": [
        "# Linear Regression from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJEzeBaYrp3a"
      },
      "source": [
        "This notebook implements a linear regression model from scratch to predict house prices. We will walk through data preprocessing, model formulation, training using gradient descent, and evaluation on a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ylSYM7srAaTK"
      },
      "outputs": [],
      "source": [
        "# Import the required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d8TzpYrAaTK"
      },
      "source": [
        "## 1. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko0DUQMIAaTL"
      },
      "source": [
        "### **1.1 Exploring the Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiIvXaDLAaTL"
      },
      "source": [
        "Let's start by loading the training data from the provided URL into a pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dj_ZCCgTAaTL"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('https://raw.githubusercontent.com/cronan03/DevSoc_AI-ML/main/train_processed_splitted.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5-4Ujl1AaTL"
      },
      "source": [
        "Let's see what the first 5 rows of this dataset look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-ieOW_eQAaTL"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LotArea</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11553</td>\n",
              "      <td>1051</td>\n",
              "      <td>1159</td>\n",
              "      <td>336</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>158000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8400</td>\n",
              "      <td>1052</td>\n",
              "      <td>1052</td>\n",
              "      <td>288</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>138500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8960</td>\n",
              "      <td>1008</td>\n",
              "      <td>1028</td>\n",
              "      <td>360</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>115000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11100</td>\n",
              "      <td>0</td>\n",
              "      <td>930</td>\n",
              "      <td>308</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>84900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15593</td>\n",
              "      <td>1304</td>\n",
              "      <td>2287</td>\n",
              "      <td>667</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>225000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   LotArea  TotalBsmtSF  GrLivArea  GarageArea  PoolArea  OverallCond  \\\n",
              "0    11553         1051       1159         336         0            5   \n",
              "1     8400         1052       1052         288         0            5   \n",
              "2     8960         1008       1028         360         0            6   \n",
              "3    11100            0        930         308         0            7   \n",
              "4    15593         1304       2287         667         0            4   \n",
              "\n",
              "  Utilities  SalePrice  \n",
              "0    AllPub     158000  \n",
              "1    AllPub     138500  \n",
              "2    AllPub     115000  \n",
              "3    AllPub      84900  \n",
              "4    AllPub     225000  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNnfJbohAaTM"
      },
      "source": [
        "What are all the features present? What is the range for each of the features along with their mean?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i0e6ZqO6AaTM"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LotArea</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1314.000000</td>\n",
              "      <td>1314.000000</td>\n",
              "      <td>1314.000000</td>\n",
              "      <td>1314.000000</td>\n",
              "      <td>1314.000000</td>\n",
              "      <td>1314.000000</td>\n",
              "      <td>1314.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>10622.104262</td>\n",
              "      <td>1058.311263</td>\n",
              "      <td>1512.900304</td>\n",
              "      <td>473.480213</td>\n",
              "      <td>2.643075</td>\n",
              "      <td>5.582192</td>\n",
              "      <td>180795.504566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>10430.181058</td>\n",
              "      <td>435.717809</td>\n",
              "      <td>524.854432</td>\n",
              "      <td>213.960987</td>\n",
              "      <td>39.504255</td>\n",
              "      <td>1.112699</td>\n",
              "      <td>77511.272784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1300.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>334.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>34900.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7588.500000</td>\n",
              "      <td>796.000000</td>\n",
              "      <td>1124.250000</td>\n",
              "      <td>336.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>130000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>9501.500000</td>\n",
              "      <td>992.000000</td>\n",
              "      <td>1461.500000</td>\n",
              "      <td>480.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>163250.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>11613.500000</td>\n",
              "      <td>1295.250000</td>\n",
              "      <td>1775.750000</td>\n",
              "      <td>576.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>215000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>215245.000000</td>\n",
              "      <td>6110.000000</td>\n",
              "      <td>5642.000000</td>\n",
              "      <td>1418.000000</td>\n",
              "      <td>738.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>755000.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             LotArea  TotalBsmtSF    GrLivArea   GarageArea     PoolArea  \\\n",
              "count    1314.000000  1314.000000  1314.000000  1314.000000  1314.000000   \n",
              "mean    10622.104262  1058.311263  1512.900304   473.480213     2.643075   \n",
              "std     10430.181058   435.717809   524.854432   213.960987    39.504255   \n",
              "min      1300.000000     0.000000   334.000000     0.000000     0.000000   \n",
              "25%      7588.500000   796.000000  1124.250000   336.000000     0.000000   \n",
              "50%      9501.500000   992.000000  1461.500000   480.000000     0.000000   \n",
              "75%     11613.500000  1295.250000  1775.750000   576.000000     0.000000   \n",
              "max    215245.000000  6110.000000  5642.000000  1418.000000   738.000000   \n",
              "\n",
              "       OverallCond      SalePrice  \n",
              "count  1314.000000    1314.000000  \n",
              "mean      5.582192  180795.504566  \n",
              "std       1.112699   77511.272784  \n",
              "min       1.000000   34900.000000  \n",
              "25%       5.000000  130000.000000  \n",
              "50%       5.000000  163250.000000  \n",
              "75%       6.000000  215000.000000  \n",
              "max       9.000000  755000.000000  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The .describe() method provides a statistical summary for all numerical columns.\n",
        "df_train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hjdd5Y5AaTM"
      },
      "source": [
        "### **1.2 Feature Scaling and One-Hot Encoding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3ls8u3hAaTM"
      },
      "source": [
        "You may have noticed that some features (like `Utilities`) are not continuous values. These are **categorical features**. We must convert them to a numerical format using **one-hot encoding**, which creates a new binary (0 or 1) column for each category.\n",
        "\n",
        "Additionally, the numerical features are on different scales (e.g., `LotArea` is in the thousands, while `OverallQual` is 1-10). This can cause features with larger scales to dominate the learning process. We will **normalize** them to a common range of $[0, 1]$ using Min-Max scaling. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeoczqQIAaTM"
      },
      "source": [
        "> **NOTE**: When you are doing feature scaling, you must store the `min` and `max` values from the **training set**. These same values must be used to normalize the test set. Why? Because the model should be evaluated on data it has never seen. Using information (like the min/max) from the test set to prepare the training data would be a form of *data leakage*, leading to an overly optimistic evaluation of the model's performance on new, unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "W17LZP2qAaTN"
      },
      "outputs": [],
      "source": [
        "# Separate the target variable (what we want to predict)\n",
        "target = 'SalePrice'\n",
        "y = df_train[target]\n",
        "\n",
        "# Separate the features\n",
        "features = df_train.drop(target, axis=1)\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "# We'll treat columns with 'object' dtype as categorical\n",
        "numerical_cols = features.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = features.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Perform one-hot encoding on categorical features\n",
        "features_encoded = pd.get_dummies(features, columns=categorical_cols, dummy_na=False)\n",
        "\n",
        "# Store the columns after encoding to align the test set later\n",
        "encoded_cols = features_encoded.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gG7rV9TCAaTN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features have been one-hot encoded and scaled.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LotArea</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>Utilities_AllPub</th>\n",
              "      <th>Utilities_NoSeWa</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.047924</td>\n",
              "      <td>0.172013</td>\n",
              "      <td>0.155426</td>\n",
              "      <td>0.236953</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.033186</td>\n",
              "      <td>0.172177</td>\n",
              "      <td>0.135268</td>\n",
              "      <td>0.203103</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.035804</td>\n",
              "      <td>0.164975</td>\n",
              "      <td>0.130746</td>\n",
              "      <td>0.253879</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.045806</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.112283</td>\n",
              "      <td>0.217207</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.750</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.066807</td>\n",
              "      <td>0.213421</td>\n",
              "      <td>0.367935</td>\n",
              "      <td>0.470381</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    LotArea  TotalBsmtSF  GrLivArea  GarageArea  PoolArea  OverallCond  \\\n",
              "0  0.047924     0.172013   0.155426    0.236953       0.0        0.500   \n",
              "1  0.033186     0.172177   0.135268    0.203103       0.0        0.500   \n",
              "2  0.035804     0.164975   0.130746    0.253879       0.0        0.625   \n",
              "3  0.045806     0.000000   0.112283    0.217207       0.0        0.750   \n",
              "4  0.066807     0.213421   0.367935    0.470381       0.0        0.375   \n",
              "\n",
              "   Utilities_AllPub  Utilities_NoSeWa  \n",
              "0              True             False  \n",
              "1              True             False  \n",
              "2              True             False  \n",
              "3              True             False  \n",
              "4              True             False  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Perform feature scaling on numerical columns\n",
        "scaler_min = features_encoded[numerical_cols].min()\n",
        "scaler_max = features_encoded[numerical_cols].max()\n",
        "\n",
        "features_encoded[numerical_cols] = (features_encoded[numerical_cols] - scaler_min) / (scaler_max - scaler_min)\n",
        "\n",
        "# Also scale the target variable, remembering its min/max for later\n",
        "y_min = y.min()\n",
        "y_max = y.max()\n",
        "y_scaled = (y - y_min) / (y_max - y_min)\n",
        "\n",
        "# Handle any potential NaN values that result from division by zero (if a column is constant)\n",
        "features_encoded.fillna(0, inplace=True)\n",
        "\n",
        "print(\"Features have been one-hot encoded and scaled.\")\n",
        "features_encoded.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcZxb-V4AaTN"
      },
      "source": [
        "### **1.3 Conversion to NumPy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-_3l_1iAaTN"
      },
      "source": [
        "Now that we have preprocessed the data, we will convert it to NumPy arrays, which are optimized for numerical computations. We will create two arrays:\n",
        "\n",
        "- **X**: The feature matrix of shape $(N, D)$, where $N$ is the number of data points and $D$ is the number of features.\n",
        "- **Y**: The target vector of shape $(N, 1)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XJk8M5QmAaTN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of feature matrix X: (1314, 9)\n",
            "Shape of target vector Y: (1314, 1)\n"
          ]
        }
      ],
      "source": [
        "# Convert the feature DataFrame and target Series to NumPy arrays\n",
        "X_train = features_encoded.to_numpy()\n",
        "y_train = y_scaled.to_numpy().reshape(-1, 1)\n",
        "\n",
        "# Add a bias term (a column of ones) to the feature matrix.\n",
        "# This allows us to treat the bias as a weight, simplifying calculations.\n",
        "# The model y = w1*x1 + ... + b becomes y = w1*x1 + ... + w0*1\n",
        "X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
        "\n",
        "print(f\"Shape of feature matrix X: {X_train.shape}\")\n",
        "print(f\"Shape of target vector Y: {y_train.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7YT1X-3AaTN"
      },
      "source": [
        "## 2. Linear Regression Formulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upgXFnXgAaTN"
      },
      "source": [
        "A linear regression model can be expressed as a single matrix equation:\n",
        "$$ Y_{pred} = XW $$\n",
        "Where:\n",
        "- $Y_{pred}$ is the $(N, 1)$ vector of predictions.\n",
        "- $X$ is the $(N, D+1)$ feature matrix (including the bias term).\n",
        "- $W$ is the $(D+1, 1)$ weight vector (including the bias weight).\n",
        "\n",
        "Let's answer the questions from the original notebook:\n",
        "- **How many parameters will we have to learn?** We will learn $D+1$ parameters, where $D$ is the number of features after one-hot encoding. This corresponds to one weight for each feature plus one bias term. In our case, this is `X_train.shape[1]`, which is **241**.\n",
        "- **Form a linear equation for our dataset:** The equation is $Y_{pred} = XW$, as shown above.\n",
        "- **Implement the linear equation:** We will now initialize the weight vector $W$ with random values and compute our initial (and likely very poor) predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IvB5DGT7AaTO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of weight vector W: (9, 1)\n",
            "Initial predictions (first 5):\n",
            "[[2.3397747257929513]\n",
            " [2.319142979411381]\n",
            " [2.4927428142227677]\n",
            " [2.562376194220425]\n",
            " [2.4355817976264755]]\n"
          ]
        }
      ],
      "source": [
        "# Get the number of data points (N) and features (D+1)\n",
        "N, D_plus_1 = X_train.shape\n",
        "\n",
        "# Initialize the weight vector W with random values from a standard normal distribution\n",
        "# The shape should be (D+1, 1) to enable matrix multiplication with X\n",
        "np.random.seed(42) # for reproducibility\n",
        "W = np.random.randn(D_plus_1, 1)\n",
        "\n",
        "# Make initial predictions\n",
        "y_pred_initial = X_train @ W\n",
        "\n",
        "print(f\"Shape of weight vector W: {W.shape}\")\n",
        "print(\"Initial predictions (first 5):\")\n",
        "print(y_pred_initial[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ldc0Ap_AaTO"
      },
      "source": [
        "How well does our model perform? Let's compare our initial predictions with the actual values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TnnX7hG5AaTO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          Prediction vs. Actual (Scaled)\n",
            "---------------------------------------------------\n",
            "Sample 1: Predicted=2.3398, Actual=0.1709\n",
            "Sample 2: Predicted=2.3191, Actual=0.1439\n",
            "Sample 3: Predicted=2.4927, Actual=0.1112\n",
            "Sample 4: Predicted=2.5624, Actual=0.0694\n",
            "Sample 5: Predicted=2.4356, Actual=0.2640\n"
          ]
        }
      ],
      "source": [
        "print(\"          Prediction vs. Actual (Scaled)\")\n",
        "print(\"---------------------------------------------------\")\n",
        "for i in range(5):\n",
        "    print(f\"Sample {i+1}: Predicted={y_pred_initial[i][0]:.4f}, Actual={y_train[i][0]:.4f}\")\n",
        "\n",
        "# As expected, the random predictions are nowhere near the actual values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MryM_jtQAaTO"
      },
      "source": [
        "### **2.1 Learning Weights using Gradient Descent**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EKd0mhkAaTO"
      },
      "source": [
        "To improve our model, we use an optimization algorithm called **Gradient Descent**. The process is:\n",
        "1.  Define a **Loss Function** ($\\mathscr{L}$) that measures how bad our predictions are.\n",
        "2.  Find the **gradients** of the loss with respect to our weights ($ \\nabla_W \\mathscr{L} $). The gradient tells us the direction of the steepest increase in the loss.\n",
        "3.  **Update the weights** by taking a small step in the *opposite* direction of the gradient: $W = W - \\alpha\\nabla_W \\mathscr{L}$, where $\\alpha$ is the learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUfs8wh3AaTO"
      },
      "source": [
        "We will use the **Mean Squared Error (MSE)** loss, which is standard for regression tasks. It assumes that the errors between predicted and true values are normally distributed.\n",
        "$$ \\mathscr{L} = \\frac{1}{N} \\sum_{i=1}^{N} (y_{pred_i} - y_{true_i})^2 $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b4G28aKRAaTO"
      },
      "outputs": [],
      "source": [
        "def mse_loss_fn(y_true, y_pred):\n",
        "    \"\"\"Calculates the Mean Squared Error loss.\"\"\"\n",
        "    return np.mean((y_pred - y_true) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUnb9WpfAaTP"
      },
      "source": [
        "Next, we calculate the gradients. The derivative of the MSE loss with respect to the entire weight vector $W$ can be calculated efficiently using matrix operations:\n",
        "$$ \\nabla_W \\mathscr{L} = \\frac{2}{N} X^T (XW - Y_{true}) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BDYpZjjMAaTP"
      },
      "outputs": [],
      "source": [
        "def get_gradients(y_true, y_pred, W, X):\n",
        "    \"\"\"\n",
        "    Calculates the gradients for the MSE loss function with respect to the weights.\n",
        "    Since the bias is included in W and X, we only need to calculate one gradient vector.\n",
        "    \"\"\"\n",
        "    N = len(y_true)\n",
        "    error = y_pred - y_true\n",
        "    dW = (2/N) * X.T @ error\n",
        "    return dW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfbjlNBXAaTP"
      },
      "source": [
        "Finally, we implement the weight update rule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9Xstl9DpAaTP"
      },
      "outputs": [],
      "source": [
        "def update(weights, gradients_weights, lr):\n",
        "    \"\"\"\n",
        "    Updates the weights using the gradients and the learning rate.\n",
        "    \"\"\"\n",
        "    weights_new = weights - lr * gradients_weights\n",
        "    return weights_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZqXDTjFAaTP"
      },
      "source": [
        "### **2.2 The Training Loop**\n",
        "\n",
        "Now we combine these functions into a loop to train the model. We'll iterate multiple times (epochs), gradually improving the weights and reducing the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "a3BVX4nlAaTP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100/1500, Loss: 0.051781\n",
            "Epoch 200/1500, Loss: 0.026920\n",
            "Epoch 300/1500, Loss: 0.017269\n",
            "Epoch 400/1500, Loss: 0.013052\n",
            "Epoch 500/1500, Loss: 0.010915\n",
            "Epoch 600/1500, Loss: 0.009653\n",
            "Epoch 700/1500, Loss: 0.008805\n",
            "Epoch 800/1500, Loss: 0.008177\n",
            "Epoch 900/1500, Loss: 0.007680\n",
            "Epoch 1000/1500, Loss: 0.007269\n",
            "Epoch 1100/1500, Loss: 0.006920\n",
            "Epoch 1200/1500, Loss: 0.006618\n",
            "Epoch 1300/1500, Loss: 0.006354\n",
            "Epoch 1400/1500, Loss: 0.006120\n",
            "Epoch 1500/1500, Loss: 0.005912\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 1500\n",
        "LEARNING_RATE = 0.1\n",
        "\n",
        "# Re-initialize weights to start training from scratch\n",
        "W = np.random.randn(D_plus_1, 1)\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # 1. Make predictions\n",
        "    y_pred = X_train @ W\n",
        "    \n",
        "    # 2. Calculate loss\n",
        "    loss = mse_loss_fn(y_train, y_pred)\n",
        "    losses.append(loss)\n",
        "    \n",
        "    # 3. Calculate gradients\n",
        "    dW = get_gradients(y_train, y_pred, W, X_train)\n",
        "    \n",
        "    # 4. Update weights\n",
        "    W = update(W, dW, LEARNING_RATE)\n",
        "    \n",
        "    # Print progress\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {loss:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYgND-OLAaTQ"
      },
      "source": [
        "Now, let's use matplotlib to plot the loss graph. A decreasing curve indicates that the model is learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RxFqG5GSAaTQ"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(losses)\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaZdAzbvAaTQ"
      },
      "source": [
        "## 3. Testing with Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuSbgtM6AaTQ"
      },
      "source": [
        "Now we evaluate our trained model on the test set. We must apply the **exact same** preprocessing steps, using the parameters (like `min`/`max` values and column order) derived from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3iuQ4ulhAaTQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test feature matrix shape: (146, 9)\n"
          ]
        }
      ],
      "source": [
        "# Load the test data\n",
        "df_test = pd.read_csv('https://raw.githubusercontent.com/cronan03/DevSoc_AI-ML/main/test_processed_splitted.csv')\n",
        "\n",
        "# Separate target and features\n",
        "y_test_orig = df_test[target]\n",
        "features_test = df_test.drop(target, axis=1)\n",
        "\n",
        "# Step 1: One-hot encode categorical features\n",
        "features_test_encoded = pd.get_dummies(features_test, columns=categorical_cols, dummy_na=False)\n",
        "\n",
        "# Step 2: Align columns with the training set\n",
        "# This adds missing columns (if any) and removes extra columns (if any)\n",
        "features_test_aligned = features_test_encoded.reindex(columns=encoded_cols, fill_value=0)\n",
        "\n",
        "# Step 3: Scale numerical features using the *training set's* min/max values\n",
        "features_test_aligned[numerical_cols] = (features_test_aligned[numerical_cols] - scaler_min) / (scaler_max - scaler_min)\n",
        "features_test_aligned.fillna(0, inplace=True)\n",
        "\n",
        "# Step 4: Scale the test target variable\n",
        "y_test_scaled = (y_test_orig - y_min) / (y_max - y_min)\n",
        "\n",
        "# Step 5: Convert to NumPy and add bias term\n",
        "X_test = features_test_aligned.to_numpy()\n",
        "X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
        "y_test = y_test_scaled.to_numpy().reshape(-1, 1)\n",
        "\n",
        "print(f\"Test feature matrix shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xvF1EJfZAaTQ"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test set using our final trained weights W\n",
        "y_pred_test_scaled = X_test @ W\n",
        "\n",
        "# Calculate the MSE loss on the scaled test data\n",
        "loss_test = mse_loss_fn(y_test, y_pred_test_scaled)\n",
        "\n",
        "# To make the results interpretable, un-scale the predictions and true values\n",
        "# back to their original dollar amounts.\n",
        "y_pred_test_unscaled = y_pred_test_scaled * (y_max - y_min) + y_min\n",
        "y_test_unscaled = y_test * (y_max - y_min) + y_min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8dfg-asd98f"
      },
      "source": [
        "### **3.1 Final Evaluation**\n",
        "\n",
        "Let's evaluate the final model and answer the questions.\n",
        "\n",
        "**- Are the predictions good?**\n",
        "We can check by comparing a random sample of predicted vs. actual prices and by looking at the test loss. The predictions should be reasonably close to the actual values.\n",
        "\n",
        "**- What is the MSE loss for the test set?**\n",
        "The calculated `loss_test` value gives us a quantitative measure of the model's performance on unseen data.\n",
        "\n",
        "**- Is the MSE loss for testing greater or lower than training?**\n",
        "Typically, the test loss is slightly **higher** than the final training loss. \n",
        "\n",
        "**- Why is this the case?**\n",
        "This is expected because the model was optimized specifically for the training data. Its performance will naturally be slightly worse on new, unseen data (the test set). A significantly higher test loss could indicate **overfitting**, where the model has learned the noise in the training data rather than the underlying patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Z-TbJp0ntTip"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         Prediction vs. Actual (Unscaled Prices)\n",
            "----------------------------------------------------------\n",
            "Sample 9: Predicted=$198279, Actual=$191000\n",
            "Sample 125: Predicted=$115119, Actual=$116900\n",
            "Sample 15: Predicted=$186691, Actual=$163000\n",
            "Sample 64: Predicted=$105931, Actual=$139000\n",
            "Sample 113: Predicted=$215531, Actual=$124900\n",
            "\n",
            "----------------------------------------------------------\n",
            "Final Training Loss (scaled): 0.005912\n",
            "Final Test Loss (scaled):     0.009776\n"
          ]
        }
      ],
      "source": [
        "# Display a random sample of 5 predictions vs actuals\n",
        "np.random.seed(10)\n",
        "sample_indices = np.random.randint(0, X_test.shape[0], 5)\n",
        "\n",
        "print(\"         Prediction vs. Actual (Unscaled Prices)\")\n",
        "print(\"----------------------------------------------------------\")\n",
        "\n",
        "for i in sample_indices:\n",
        "    pred = int(y_pred_test_unscaled[i][0])\n",
        "    actual = int(y_test_unscaled[i][0])\n",
        "    print(f\"Sample {i}: Predicted=${pred}, Actual=${actual}\")\n",
        "\n",
        "print(\"\\n----------------------------------------------------------\")\n",
        "print(f\"Final Training Loss (scaled): {losses[-1]:.6f}\")\n",
        "print(f\"Final Test Loss (scaled):     {loss_test:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
